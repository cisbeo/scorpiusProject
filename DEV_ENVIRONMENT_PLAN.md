# ğŸ› ï¸ Plan d'Environnement de DÃ©veloppement - Scorpius Project

## ğŸ¯ Objectif
Configurer un environnement de dÃ©veloppement complet permettant d'implÃ©menter efficacement les nouvelles fonctionnalitÃ©s et de rÃ©aliser des tests d'intÃ©gration robustes.

## ğŸ“‹ Analyse de l'existant

### âœ… Ce qui est dÃ©jÃ  configurÃ©
- Docker Compose local avec PostgreSQL + Redis + pgAdmin
- Configuration SQLite pour tests rapides
- Scripts de test automatisÃ©s (`test_local.sh`)
- Environnement de variables (.env.local)
- Tests d'intÃ©gration de base
- Outils de qualitÃ© de code (ruff, mypy, bandit)

### ğŸ”´ Points Ã  amÃ©liorer
- Pas de hot-reload pour le dÃ©veloppement
- Tests d'intÃ©gration incomplets
- Environnement de debug limitÃ©
- Pas de mock des services externes
- Documentation dev incomplÃ¨te

## ğŸ—ï¸ Architecture de dÃ©veloppement recommandÃ©e

### Option 1: DÃ©veloppement local avec SQLite (rapide)
```
Local Machine
â”œâ”€â”€ FastAPI (uvicorn --reload)
â”œâ”€â”€ SQLite (test_scorpius.db)
â”œâ”€â”€ Uploads locaux (./uploads)
â””â”€â”€ Logs locaux (./logs)
```

### Option 2: DÃ©veloppement avec Docker (plus proche de prod)
```
Docker Compose
â”œâ”€â”€ API Container (volume mounted)
â”œâ”€â”€ PostgreSQL Container
â”œâ”€â”€ Redis Container
â”œâ”€â”€ pgAdmin Container
â””â”€â”€ Shared volumes
```

## ğŸ”§ Configuration recommandÃ©e

### 1. Setup de base pour dÃ©veloppement

#### Structure des environnements
```bash
# Environnements multiples
.env.local          # SQLite + dÃ©veloppement rapide
.env.docker         # Docker Compose local
.env.test           # Tests automatisÃ©s
.env.staging        # PrÃ©-production
.env.prod           # Production
```

#### Scripts de dÃ©veloppement
```bash
# Scripts utilitaires
scripts/
â”œâ”€â”€ dev-setup.sh         # Installation environnement
â”œâ”€â”€ dev-start.sh         # DÃ©marrage mode dev
â”œâ”€â”€ dev-test.sh          # Tests complets
â”œâ”€â”€ dev-reset.sh         # Reset environnement
â””â”€â”€ dev-lint.sh          # QualitÃ© code
```

### 2. Configuration IDE et outils

#### VSCode (recommandÃ©)
```json
// .vscode/settings.json
{
    "python.defaultInterpreterPath": "./venv/bin/python",
    "python.linting.enabled": true,
    "python.linting.ruffEnabled": true,
    "python.formatting.provider": "black",
    "python.testing.pytestEnabled": true,
    "python.testing.pytestArgs": ["tests/"],
    "files.exclude": {
        "**/__pycache__": true,
        "**/*.pyc": true,
        ".pytest_cache": true,
        ".ruff_cache": true,
        ".mypy_cache": true
    }
}
```

#### Extensions VSCode recommandÃ©es
- Python
- Pylance
- Ruff
- Docker
- REST Client
- GitLens
- Thunder Client (tests API)

### 3. Hot-reload et dÃ©veloppement continu

#### Configuration uvicorn pour dÃ©veloppement
```python
# dev_server.py
import uvicorn
from pathlib import Path

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        reload_dirs=["src"],  # Surveiller seulement src/
        log_level="debug",
        access_log=True,
        env_file=".env.local"
    )
```

#### Watch files pour auto-restart
```bash
# Avec watchdog pour surveillance fichiers
pip install watchdog[watchmedo]

# Auto-restart sur changement
watchmedo auto-restart --pattern="*.py" --recursive -- python dev_server.py
```

## ğŸ§ª Configuration des tests

### 1. Tests d'intÃ©gration Ã©tendus

#### Structure des tests
```
tests/
â”œâ”€â”€ unit/                    # Tests unitaires
â”‚   â”œâ”€â”€ test_auth.py
â”‚   â”œâ”€â”€ test_documents.py
â”‚   â””â”€â”€ test_company.py
â”œâ”€â”€ integration/             # Tests d'intÃ©gration API
â”‚   â”œâ”€â”€ test_auth_flow.py
â”‚   â”œâ”€â”€ test_document_upload.py
â”‚   â”œâ”€â”€ test_company_crud.py
â”‚   â””â”€â”€ test_capabilities.py
â”œâ”€â”€ e2e/                     # Tests end-to-end
â”‚   â”œâ”€â”€ test_complete_workflow.py
â”‚   â””â”€â”€ test_user_journey.py
â”œâ”€â”€ fixtures/                # DonnÃ©es de test
â”‚   â”œâ”€â”€ test_documents/
â”‚   â””â”€â”€ test_data.py
â””â”€â”€ conftest.py             # Configuration pytest
```

#### Configuration pytest avancÃ©e
```python
# conftest.py
import pytest
import asyncio
from httpx import AsyncClient
from fastapi.testclient import TestClient
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

from main import app
from src.db.base import Base
from src.db.session import get_async_db

# Base de donnÃ©es test en mÃ©moire
TEST_DATABASE_URL = "sqlite+aiosqlite:///:memory:"

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
async def async_db():
    """Create test database."""
    engine = create_async_engine(TEST_DATABASE_URL, echo=False)
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
    async with async_session() as session:
        yield session

    await engine.dispose()

@pytest.fixture
async def client(async_db):
    """Create test client with database override."""
    app.dependency_overrides[get_async_db] = lambda: async_db
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac
    app.dependency_overrides.clear()

@pytest.fixture
def test_user_data():
    """Sample user data for tests."""
    return {
        "email": "test@example.com",
        "password": "TestPassword123!",
        "full_name": "Test User",
        "role": "bid_manager"
    }

@pytest.fixture
async def authenticated_user(client, test_user_data):
    """Create and authenticate a test user."""
    # Register user
    await client.post("/api/v1/auth/register", json=test_user_data)

    # Login
    response = await client.post("/api/v1/auth/login", json={
        "email": test_user_data["email"],
        "password": test_user_data["password"]
    })

    token = response.json()["data"]["access_token"]
    return {"token": token, "user_data": test_user_data}
```

### 2. Tests de performance et charge

#### Configuration des tests de performance
```python
# tests/performance/test_load.py
import pytest
import asyncio
from httpx import AsyncClient
import time

@pytest.mark.performance
async def test_concurrent_users():
    """Test with multiple concurrent users."""
    async def make_request():
        async with AsyncClient(base_url="http://localhost:8000") as client:
            response = await client.get("/health")
            return response.status_code == 200

    # Simuler 50 utilisateurs concurrents
    tasks = [make_request() for _ in range(50)]
    start_time = time.time()
    results = await asyncio.gather(*tasks)
    duration = time.time() - start_time

    assert all(results), "Certaines requÃªtes ont Ã©chouÃ©"
    assert duration < 5.0, f"Temps de rÃ©ponse trop long: {duration}s"

@pytest.mark.performance
async def test_document_upload_performance():
    """Test upload performance with large files."""
    # Test avec fichiers de diffÃ©rentes tailles
    pass
```

### 3. Mock et simulation

#### Mock des services externes
```python
# tests/mocks/external_services.py
from unittest.mock import AsyncMock, patch
import pytest

@pytest.fixture
def mock_email_service():
    """Mock email service."""
    with patch('src.services.email_service.EmailService') as mock:
        mock.send_email = AsyncMock(return_value=True)
        yield mock

@pytest.fixture
def mock_document_processor():
    """Mock document processing."""
    with patch('src.processors.pdf_processor.PDFProcessor') as mock:
        mock.extract_text = AsyncMock(return_value="Mocked text content")
        mock.extract_metadata = AsyncMock(return_value={"pages": 5})
        yield mock
```

## ğŸ” Debugging et monitoring

### 1. Configuration logging dÃ©veloppement

#### Logging structurÃ© pour debug
```python
# src/core/logging.py
import logging
import sys
from datetime import datetime
from typing import Dict, Any

class DevFormatter(logging.Formatter):
    """Formatter spÃ©cialisÃ© pour dÃ©veloppement."""

    def format(self, record):
        # Couleurs pour terminal
        colors = {
            'DEBUG': '\033[36m',     # Cyan
            'INFO': '\033[32m',      # Vert
            'WARNING': '\033[33m',   # Jaune
            'ERROR': '\033[31m',     # Rouge
            'CRITICAL': '\033[35m',  # Magenta
        }

        color = colors.get(record.levelname, '')
        reset = '\033[0m'

        timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S.%f')[:-3]

        return f"{color}[{timestamp}] {record.levelname:8} | {record.name:20} | {record.getMessage()}{reset}"

def setup_dev_logging():
    """Configuration logging pour dÃ©veloppement."""
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)

    # Handler console avec couleurs
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(DevFormatter())
    console_handler.setLevel(logging.DEBUG)

    # Handler fichier pour debug
    file_handler = logging.FileHandler('logs/dev_debug.log')
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s'
    ))
    file_handler.setLevel(logging.DEBUG)

    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)

    # RÃ©duire verbositÃ© des libraries externes
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)
```

### 2. Profiling et performance

#### Profiling automatique
```python
# src/middleware/profiling.py
import cProfile
import pstats
import io
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware

class ProfilingMiddleware(BaseHTTPMiddleware):
    """Middleware de profiling pour dÃ©veloppement."""

    async def dispatch(self, request: Request, call_next):
        if request.url.path.startswith("/profile/"):
            # Profiling activÃ© pour ces endpoints
            profiler = cProfile.Profile()
            profiler.enable()

            response = await call_next(request)

            profiler.disable()

            # Sauvegarder stats
            s = io.StringIO()
            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
            ps.print_stats()

            # Ajouter dans headers pour debug
            response.headers["X-Profile-Stats"] = s.getvalue()[:1000]

            return response
        else:
            return await call_next(request)
```

## ğŸš€ Workflow de dÃ©veloppement

### 1. DÃ©marrage rapide quotidien

#### Script dev-start.sh
```bash
#!/bin/bash
# scripts/dev-start.sh

set -e

echo "ğŸš€ DÃ©marrage environnement Scorpius Dev"

# Activer environnement virtuel
if [[ ! -d "venv" ]]; then
    echo "CrÃ©ation environnement virtuel..."
    python3 -m venv venv
fi

source venv/bin/activate

# Installer/mettre Ã  jour dÃ©pendances
echo "ğŸ“¦ Mise Ã  jour dÃ©pendances..."
pip install -r requirements.txt -r requirements-dev.txt

# CrÃ©er dossiers nÃ©cessaires
mkdir -p uploads temp logs

# VÃ©rifier base de donnÃ©es
if [[ ! -f "test_scorpius.db" ]]; then
    echo "ğŸ—„ï¸ Initialisation base de donnÃ©es..."
    python scripts/init_local_db.py
fi

# DÃ©marrer serveur de dÃ©veloppement
echo "ğŸŒŸ DÃ©marrage serveur..."
echo "API: http://localhost:8000"
echo "Docs: http://localhost:8000/docs"
echo "Health: http://localhost:8000/health"
echo ""

python dev_server.py
```

### 2. Tests automatisÃ©s continus

#### Configuration GitHub Actions pour dev
```yaml
# .github/workflows/dev-tests.yml
name: Dev Tests

on:
  push:
    branches: [ develop, feature/* ]
  pull_request:
    branches: [ develop, main ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run linting
      run: |
        ruff check src tests
        mypy src

    - name: Run tests
      run: |
        pytest tests/ -v --cov=src --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

### 3. Pre-commit hooks avancÃ©s

#### Configuration .pre-commit-config.yaml Ã©tendue
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-json
      - id: check-merge-conflict
      - id: check-docstring-first
      - id: debug-statements

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.0
    hooks:
      - id: ruff
        args: [ --fix, --exit-non-zero-on-fix ]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]

  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.5
    hooks:
      - id: bandit
        args: ['-r', 'src/']

  - repo: local
    hooks:
      - id: pytest-fast
        name: Fast tests
        entry: pytest tests/unit -x
        language: system
        pass_filenames: false
        always_run: true
```

## ğŸ“Š MÃ©triques et monitoring dev

### 1. Dashboard dÃ©veloppement

#### Endpoint de mÃ©triques dev
```python
# src/api/dev/metrics.py
from fastapi import APIRouter, Depends
from src.core.auth import get_current_user
import psutil
import time

router = APIRouter(prefix="/dev", tags=["development"])

@router.get("/metrics")
async def get_dev_metrics():
    """MÃ©triques pour dÃ©veloppement uniquement."""
    return {
        "server": {
            "uptime": time.time() - start_time,
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent
        },
        "database": {
            "connections": await get_db_connection_count(),
            "queries_count": await get_queries_count()
        },
        "api": {
            "endpoints_hit": endpoint_stats,
            "average_response_time": response_times.average()
        }
    }

@router.get("/logs/tail")
async def tail_logs(lines: int = 50):
    """DerniÃ¨res lignes de logs."""
    with open('logs/dev_debug.log', 'r') as f:
        return {"logs": f.readlines()[-lines:]}
```

## ğŸ¯ Objectifs et KPIs

### Objectifs de l'environnement de dÃ©veloppement
1. **Temps de setup** < 5 minutes pour nouvel dÃ©veloppeur
2. **Hot-reload** < 2 secondes aprÃ¨s modification
3. **Tests complets** < 30 secondes
4. **Coverage** > 80% pour toutes les fonctionnalitÃ©s
5. **Feedback temps rÃ©el** sur qualitÃ© code

### MÃ©triques de succÃ¨s
- âœ… Tests d'intÃ©gration passent Ã  100%
- âœ… Nouveaux endpoints testÃ©s automatiquement
- âœ… Debug facile avec logs structurÃ©s
- âœ… Performance monitorÃ©e en continu
- âœ… Documentation Ã  jour automatiquement

## ğŸ“š Documentation et resources

### Documentation automatique
- OpenAPI/Swagger toujours Ã  jour
- Docstrings complÃ¨tes avec exemples
- Tests comme documentation vivante
- Architecture Decision Records (ADR)

### Resources utiles
- [FastAPI Best Practices](https://fastapi-best-practices.netlify.app/)
- [AsyncIO Testing](https://docs.python.org/3/library/asyncio-dev.html)
- [SQLAlchemy Async](https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)

---

**âœ… Plan d'environnement de dÃ©veloppement prÃªt pour implÃ©mentation**

Ce plan couvre tous les aspects pour un dÃ©veloppement efficace et des tests d'intÃ©gration robustes.